{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin Toss Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "import numpy as np\n",
    "import copy \n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import binom\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finite_q_learning import *\n",
    "from q_learning import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of our asymetrical ambiguity sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5)\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "def P2_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.6)\n",
    "def p2_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.6)\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0    # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1  # Epsilon greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computation of the useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the functions that allow us to get the index of an element a (reps. x) in A (resp. X)\n",
    "if np.ndim(A) > 1:\n",
    "    A_list = A\n",
    "else:\n",
    "    A_list = np.array([[a] for a in A])\n",
    "if np.ndim(X) > 1:\n",
    "    X_list = X\n",
    "else:\n",
    "    X_list = np.array([[x] for x in X])\n",
    "\n",
    "def a_index(a):\n",
    "    return np.flatnonzero((a==A_list).all(1))[0]\n",
    "def x_index(x):\n",
    "    return np.flatnonzero((x==X_list).all(1))[0]\n",
    "\n",
    "# Get the result of the Q-Learning algorithm,\n",
    "# Get the optimal results for each x in X\n",
    "def a_opt(x, Q_opt):\n",
    "    return A[np.argmax(Q_opt[x_index(x),:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run for the ambiguity set $P_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_opt_robust_, Visits = finite_q_learning(X, A, r, np.array([P1_0, P2_0]), np.array([p1_0, p2_0]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_06 = [1, 1, 1, 1, 1, 0, 0, -1, -1, -1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run for the ambiguity set $P_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "def P1_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.5)\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "def P2_0(x,a):\n",
    "    return binom.rvs(nr_coins, 0.3)\n",
    "def p2_0(x,a,y):\n",
    "    return binom.pmf(y,nr_coins,0.3)\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "k_0        = 0    # Initial index of the corresponding MDP, starting with the central proba of 1/2\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_opt_robust_, Visits = finite_q_learning(X, A, r, np.array([P1_0, P2_0]), np.array([p1_0, p2_0]), alpha, x_0, k_0, eps_greedy, Nr_iter, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_03 = [1, 1, 1, 0, 0, 0, -1, -1, -1, -1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run of the Wasserstein algorithm for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wasserstein_q_learning import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS  = [1, 2]\n",
    "    \n",
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def c(x, y):\n",
    "    return np.abs(x-y)\n",
    "\n",
    "def r(x, a, y):\n",
    "    return(a * (y > x) - a * (y < x) - np.abs(a) * (x == y)) # Reward function\n",
    "\n",
    "def P1_0(x, a):\n",
    "    return binom.rvs(nr_coins, 0.5) # Assumption that is a fair coin\n",
    "def p1_0(x,a,y):\n",
    "    return binom.pmf(y, nr_coins,0.5)\n",
    "\n",
    "\n",
    "# Adding some robustness to the model of a \"fair coin\"\n",
    "epsilon = EPS[0]\n",
    "\n",
    "alpha      = 0.95 # Discount Factor\n",
    "x_0        = 5    # Initial Value\n",
    "eps_greedy = 0.1  # Epsilon greedy policy\n",
    "\n",
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_opt_robust_ws = wasserstein_q_learning(X, A, r, c, P1_0, p1_0, epsilon, alpha, x_0, eps_greedy, Nr_iter, q = 1, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_ws) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_eps1 = [1, 1, 1, 1, 0, 0, 0, -1, -1, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = EPS[1]\n",
    "\n",
    "Nr_iter = 1_000_000\n",
    "\n",
    "Q_opt_robust_ws = wasserstein_q_learning(X, A, r, c, P1_0, p1_0, epsilon, alpha, x_0, eps_greedy, Nr_iter, q = 1, gamma_t_tilde = lambda t: 1/(t+1), Q_0 = np.ones([len(X),len(A)]))\n",
    "\n",
    "df = pd.DataFrame(np.array([[a_opt(x, Q_opt_robust_ws) for x in X]]))\n",
    "df[\"State\"]=[\"Robust, finite spaces\"]\n",
    "df = df.set_index(\"State\").reset_index()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_eps2 = [1, 1, 1, 0, 0, 0, 0, 0, -1, -1, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_coins = 10\n",
    "X        = np.linspace(0, nr_coins, nr_coins+1)        # States\n",
    "A        = np.array([-1, 0, 1])                        # Actions\n",
    "\n",
    "def r(x,a,y):\n",
    "    return(a * (y>x) - a * (y<x) - np.abs(a) * (x==y)) # Reward function\n",
    "\n",
    "Nr_iter = 100_000\n",
    "\n",
    "alpha = 0.95\n",
    "x_0   = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_nonrobust = np.array([1, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1])\n",
    "\n",
    "policies = np.array([a_06, a_03, a_eps1, a_eps2, a_nonrobust])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tilde = np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = []\n",
    "for a_tilde in policies:\n",
    "\n",
    "    cr_p = []\n",
    "    for p in p_tilde:\n",
    "\n",
    "        def P(x,a):\n",
    "            return binom.rvs(nr_coins, p)\n",
    "\n",
    "        E = 0\n",
    "        x = x_0\n",
    "        for n in range(Nr_iter):\n",
    "    \n",
    "            a = a_tilde[x]\n",
    "            y = P(x,a)\n",
    "            E += r(x, a, y)\n",
    "\n",
    "            x = y\n",
    "\n",
    "        cr_p += [E]\n",
    "\n",
    "    CR += [cr_p]\n",
    "\n",
    "CR = np.array(CR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
